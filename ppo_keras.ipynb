{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sh2\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import keras.layers as layers\n",
    "import keras.optimizers as optimizers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self,input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.__make_network()\n",
    "        self.__make_loss_function()\n",
    "        \n",
    "    def __make_network(self):\n",
    "        input_layer = layers.Input(shape=(self.input_shape,))\n",
    "        x = layers.Dense(256,activation = 'relu')(input_layer)\n",
    "        #x = layers.Dense(256,activation = 'relu')(x)\n",
    "        x = layers.Dense(1)(x)\n",
    "        self.model = Model(inputs = input_layer, outputs = x)\n",
    "    def get_value(self,state):\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def __make_loss_function(self):\n",
    "        \n",
    "        value_output = self.model.output\n",
    "        reward_placeholder = K.placeholder(shape=(None,1),name = 'reward')\n",
    "        #HUBER_DELTA = 0.5\n",
    "        #loss = K.abs(reward_placeholder - value_output)\n",
    "        #loss = K.switch(loss < HUBER_DELTA, 0.5 * loss ** 2 , HUBER_DELTA * (loss - 0.5 * HUBER_DELTA))\n",
    "        #loss = K.sum(loss)\n",
    "        loss = K.mean(K.square(reward_placeholder - value_output))\n",
    "        \n",
    "        optimizer = optimizers.Adam(learning_rate)\n",
    "        update = optimizer.get_updates(loss =loss, params = self.model.trainable_weights)\n",
    "        \n",
    "        self.update_function = K.function(inputs = [self.model.input,\\\n",
    "                                                   reward_placeholder],\\\n",
    "                                         outputs = [] , updates = update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self,input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        self.__make_network()\n",
    "        self.__make_loss_function()\n",
    "        \n",
    "    def __make_network(self):\n",
    "        input_layer = layers.Input(shape=(self.input_shape,))    \n",
    "        x = layers.Dense(256, activation = 'relu')(input_layer)\n",
    "        #x = layers.Dense(256, activation = 'relu')(x)\n",
    "        x = layers.Dense(self.output_shape, activation='softmax')(x)\n",
    "        self.model = Model(inputs = input_layer, outputs = x)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def __make_loss_function(self):\n",
    "        before_action_prob = K.placeholder(shape = (None, 1),\\\n",
    "                                          name = 'before_action_prob')\n",
    "        before_action = K.placeholder(shape = (None, 1),\\\n",
    "                                          name = 'before_action',dtype = 'int64') ########\n",
    "\n",
    "        advantage = K.placeholder(shape = (None,1), name ='advantage')\n",
    "        \n",
    "        now_action_prob = self.model.output\n",
    "        now_action_select = K.sum(tf.squeeze(tf.one_hot(before_action,depth=2),axis=1) * now_action_prob,axis=-1)\n",
    "        now_action_select = K.reshape(now_action_select,(-1,1))\n",
    "        \n",
    "        ratio = (K.exp(K.log(now_action_select) - K.log(before_action_prob)))\n",
    "        \n",
    "        #return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, \\\n",
    "        #max_value=1 + LOSS_CLIPPING) * advantage) + ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))\n",
    "        \n",
    "        surr_1 = ratio * advantage\n",
    "        surr_2 = K.clip(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "        loss = - K.mean(K.minimum(surr_1,surr_2))\n",
    "        optimizer = optimizers.Adam(lr = learning_rate)\n",
    "        updates = optimizer.get_updates(loss = loss, params = self.model.trainable_weights)\n",
    "        #[state,action,prob,advantage]\n",
    "        self.update_function = K.function(inputs = [self.model.input,before_action,\\\n",
    "                                       before_action_prob,advantage],\\\n",
    "                            outputs = [ratio],\\\n",
    "                            updates = updates)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, input_shape,output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        self.actor = Actor(input_shape,output_shape)\n",
    "        self.value = Value(input_shape)\n",
    "    \n",
    "        self.memory = []\n",
    "    def put_data(self,data):\n",
    "        self.memory.append(data)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        return self.actor.model.predict(state)\n",
    "    def memory_to_trainable(self):\n",
    "        state_list, action_list, reward_list, next_state_list, prob_list, done_list = [],\\\n",
    "        [], [], [], [], []\n",
    "        \n",
    "        for data in self.memory:\n",
    "            state, action, reward, next_state, prob, done = data\n",
    "            \n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            reward_list.append([reward])\n",
    "            next_state_list.append(next_state)\n",
    "            prob_list.append([prob])\n",
    "            done = 0 if done else 1\n",
    "            done_list.append([done])\n",
    "        return np.array(state_list), np.array(action_list), np.array(reward_list),\\\n",
    "                np.array(next_state_list), np.array(prob_list), np.array(done_list)     \n",
    "        \n",
    "    def train(self):\n",
    "        state,action,reward,next_state,prob,done_mask = self.memory_to_trainable()\n",
    "        done_mask = done_mask.reshape(-1,1)\n",
    "        #print('state',state.shape)\n",
    "        #print('action',action.shape)\n",
    "        #print('reward',reward.shape)\n",
    "        #print('next_state',next_state.shape)\n",
    "        #print('prob',prob.shape)\n",
    "        #print('done_mask',done_mask.shape)\n",
    "        for i in range(env_iteration_number):\n",
    "            #print('train iterate : ',i)\n",
    "            td_error = reward + gamma * self.value.get_value(next_state) * done_mask\n",
    "            delta = np.array(td_error - self.value.get_value(state))\n",
    "            advantage_list = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_list.append(advantage)\n",
    "            advantage_list.reverse()\n",
    "            advantage = np.array(advantage_list).reshape(-1,1)\n",
    "            #print('advantage',advantage)\n",
    "            (self.actor.update_function([state,action,prob,advantage]))\n",
    "            self.value.update_function([state,advantage])\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env_iteration_number = 3\n",
    "learning_rate = 0.0005\n",
    "lmbda =0.95\n",
    "eps_clip = 0.1\n",
    "epochs = 100\n",
    "T_horizon = 20\n",
    "gamma = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Agent(env.observation_space.shape[0],env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20  episode average :  21.6\n",
      "40  episode average :  29.6\n",
      "60  episode average :  31.8\n",
      "80  episode average :  33.65\n",
      "100  episode average :  44.75\n",
      "120  episode average :  63.9\n",
      "140  episode average :  61.0\n",
      "160  episode average :  147.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-36272795aa3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miterate\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m20\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miterate\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" episode average : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f1dac5847e4d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0madvantage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madvantage_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m#print('advantage',advantage)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madvantage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madvantage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sh2\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sh2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sh2\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1104\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mc:\\users\\sh2\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "check = 20\n",
    "score = 0\n",
    "\n",
    "for iterate in range(2000):\n",
    "    #print(iterate)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        for t in range(T_horizon):\n",
    "            action_prob = model.get_action(state.reshape(1,-1))\n",
    "\n",
    "            action = np.random.choice([x for x in range(env.action_space.n)], p = action_prob[0])\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            model.put_data((state,action,reward/100.0, next_state,action_prob[0][action],done))\n",
    "            state = next_state\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        model.train()\n",
    "    if (iterate % 20 == 0) & (iterate != 0) :\n",
    "        print(iterate, \" episode average : \", score / check) \n",
    "        score =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7360866  0.26391342] 0 False\n",
      "[0.4725775 0.5274225] 1 False\n",
      "[0.74555475 0.25444523] 0 False\n",
      "[0.4822357 0.5177644] 0 False\n",
      "[0.24546207 0.75453794] 1 False\n",
      "[0.46971887 0.5302812 ] 0 False\n",
      "[0.23232603 0.767674  ] 1 False\n",
      "[0.4454279 0.5545721] 1 False\n",
      "[0.7149244 0.2850756] 1 False\n",
      "[0.87187034 0.12812963] 0 False\n",
      "[0.7154518 0.2845482] 0 False\n",
      "[0.4318987 0.5681012] 0 False\n",
      "[0.20463656 0.7953635 ] 0 False\n",
      "[0.08510637 0.9148937 ] 1 False\n",
      "[0.16999276 0.8300072 ] 0 False\n",
      "[0.06805086 0.9319491 ] 1 False\n",
      "[0.13081755 0.8691824 ] 1 False\n",
      "[0.25328594 0.74671406] 1 False\n",
      "[0.46573368 0.5342663 ] 0 False\n",
      "[0.20069928 0.7993007 ] 1 False\n",
      "[0.3809905 0.6190095] 1 False\n",
      "[0.62185293 0.37814713] 0 False\n",
      "[0.31133687 0.6886631 ] 1 False\n",
      "[0.547242 0.452758] 0 False\n",
      "[0.24823552 0.75176454] 0 False\n",
      "[0.09568042 0.9043196 ] 1 False\n",
      "[0.18162803 0.81837195] 1 False\n",
      "[0.35103127 0.6489687 ] 1 False\n",
      "[0.5680176  0.43198237] 1 False\n",
      "[0.74437106 0.2556289 ] 0 False\n",
      "[0.5013852  0.49861476] 0 False\n",
      "[0.22405942 0.77594054] 1 False\n",
      "[0.42169502 0.578305  ] 0 False\n",
      "[0.17303847 0.8269616 ] 0 False\n",
      "[0.07689326 0.92310673] 1 False\n",
      "[0.12706602 0.87293404] 1 False\n",
      "[0.24617563 0.75382435] 1 False\n",
      "[0.43040812 0.56959194] 0 False\n",
      "[0.18570934 0.81429064] 1 False\n",
      "[0.34494868 0.6550513 ] 1 False\n",
      "[0.5418729  0.45812705] 1 False\n",
      "[0.7088802  0.29111984] 0 False\n",
      "[0.4868153  0.51318467] 1 False\n",
      "[0.67015237 0.3298476 ] 1 False\n",
      "[0.8073567  0.19264328] 0 False\n",
      "[0.6492515 0.3507485] 0 False\n",
      "[0.42379194 0.576208  ] 1 False\n",
      "[0.6278214  0.37217864] 0 False\n",
      "[0.40296894 0.5970311 ] 0 False\n",
      "[0.19471374 0.80528635] 1 False\n",
      "[0.3738004 0.6261996] 1 False\n",
      "[0.58412105 0.41587892] 0 False\n",
      "[0.3590235 0.6409765] 0 False\n",
      "[0.17287515 0.8271249 ] 1 False\n",
      "[0.3353999  0.66460013] 1 False\n",
      "[0.5473279  0.45267203] 1 False\n",
      "[0.7339734  0.26602668] 0 False\n",
      "[0.555258   0.44474193] 0 False\n",
      "[0.33856905 0.661431  ] 1 False\n",
      "[0.55928737 0.44071266] 1 False\n",
      "[0.7496184  0.25038153] 0 False\n",
      "[0.58477247 0.41522747] 0 False\n",
      "[0.37842932 0.6215707 ] 1 False\n",
      "[0.60631    0.39369002] 1 False\n",
      "[0.78765047 0.21234958] 1 False\n",
      "[0.89621514 0.10378486] 1 False\n",
      "[0.95005697 0.04994295] 0 False\n",
      "[0.92267066 0.07732934] 1 False\n",
      "[0.9625977  0.03740226] 0 False\n",
      "[0.9455519  0.05444809] 0 False\n",
      "[0.9208228  0.07917719] 0 False\n",
      "[0.88491476 0.11508518] 0 False\n",
      "[0.8313567  0.16864327] 0 False\n",
      "[0.7407424  0.25925767] 0 False\n",
      "[0.60778433 0.3922157 ] 0 False\n",
      "[0.4555558 0.5444442] 0 False\n",
      "[0.3097731 0.6902269] 0 False\n",
      "[0.1873587  0.81264126] 0 False\n",
      "[0.1032135  0.89678645] 1 False\n",
      "[0.21401604 0.785984  ] 1 False\n",
      "[0.40620247 0.59379756] 1 False\n",
      "[0.64198995 0.35801008] 0 False\n",
      "[0.45873424 0.5412657 ] 1 False\n",
      "[0.69068855 0.30931142] 0 False\n",
      "[0.50992507 0.49007493] 1 False\n",
      "[0.7324173  0.26758274] 0 False\n",
      "[0.5596921  0.44030786] 0 False\n",
      "[0.36676154 0.63323843] 0 False\n",
      "[0.19791614 0.8020839 ] 1 False\n",
      "[0.3758213  0.62417877] 1 False\n",
      "[0.5949392  0.40506083] 0 False\n",
      "[0.3864923 0.6135076] 1 False\n",
      "[0.60326165 0.39673835] 1 False\n",
      "[0.7878144  0.21218559] 1 False\n",
      "[0.8972598  0.10274021] 0 False\n",
      "[0.8103422 0.1896578] 0 False\n",
      "[0.65986097 0.34013906] 0 False\n",
      "[0.45274532 0.5472547 ] 1 False\n",
      "[0.6712416  0.32875842] 0 False\n",
      "[0.4615233 0.5384767] 0 False\n",
      "[0.25734 0.74266] 0 False\n",
      "[0.12012052 0.8798795 ] 1 False\n",
      "[0.22945493 0.77054507] 1 False\n",
      "[0.401039 0.598961] 0 False\n",
      "[0.20158829 0.79841167] 1 False\n",
      "[0.35481086 0.64518917] 1 False\n",
      "[0.54493755 0.45506248] 0 False\n",
      "[0.31345433 0.68654567] 1 False\n",
      "[0.49159008 0.50840986] 0 False\n",
      "[0.2683264  0.73167354] 0 False\n",
      "[0.11909576 0.88090426] 1 False\n",
      "[0.21016452 0.78983545] 1 False\n",
      "[0.34096947 0.65903056] 1 False\n",
      "[0.5129512  0.48704877] 1 False\n",
      "[0.688998 0.311002] 0 False\n",
      "[0.4519056 0.5480944] 0 False\n",
      "[0.23921375 0.76078624] 1 False\n",
      "[0.3766873 0.6233127] 1 False\n",
      "[0.55205846 0.4479415 ] 0 False\n",
      "[0.3168149  0.68318504] 0 False\n",
      "[0.15446503 0.8455349 ] 1 False\n",
      "[0.25006837 0.74993163] 0 False\n",
      "[0.11592367 0.88407636] 1 False\n",
      "[0.18707654 0.81292343] 1 False\n",
      "[0.29088432 0.70911574] 0 False\n",
      "[0.13971041 0.8602896 ] 1 False\n",
      "[0.21916889 0.78083116] 1 False\n",
      "[0.339528 0.660472] 0 False\n",
      "[0.16629182 0.83370817] 1 False\n",
      "[0.25728828 0.7427117 ] 1 False\n",
      "[0.40375888 0.5962411 ] 0 False\n",
      "[0.199614 0.800386] 0 False\n",
      "[0.09606629 0.9039337 ] 1 False\n",
      "[0.14591083 0.8540892 ] 1 False\n",
      "[0.23061742 0.76938254] 1 False\n",
      "[0.36778775 0.6322122 ] 1 False\n",
      "[0.53251064 0.46748933] 1 False\n",
      "[0.69055897 0.309441  ] 0 False\n",
      "[0.48278284 0.51721716] 1 False\n",
      "[0.65393233 0.34606767] 0 False\n",
      "[0.44552913 0.5544709 ] 1 False\n",
      "[0.6263055  0.37369457] 1 False\n",
      "[0.77363324 0.2263668 ] 0 False\n",
      "[0.6242644  0.37573564] 1 False\n",
      "[0.77741057 0.22258942] 0 False\n",
      "[0.63901794 0.36098206] 1 False\n",
      "[0.79251236 0.20748761] 0 False\n",
      "[0.66891426 0.33108577] 0 False\n",
      "[0.49807307 0.50192696] 0 False\n",
      "[0.3199709  0.68002903] 1 False\n",
      "[0.52686995 0.47313005] 0 False\n",
      "[0.34832472 0.6516753 ] 1 False\n",
      "[0.56044614 0.43955386] 1 False\n",
      "[0.7487239  0.25127608] 0 False\n",
      "[0.6151092  0.38489088] 1 False\n",
      "[0.78732854 0.21267143] 0 False\n",
      "[0.6784341 0.3215659] 0 False\n",
      "[0.52569    0.47430995] 0 False\n",
      "[0.37899137 0.62100863] 1 False\n",
      "[0.5836864  0.41631362] 1 False\n",
      "[0.7710706 0.2289294] 0 False\n",
      "[0.65565956 0.34434044] 0 False\n",
      "[0.51576406 0.48423603] 1 False\n",
      "[0.71565324 0.28434682] 0 False\n",
      "[0.5890993  0.41090068] 0 False\n",
      "[0.45924076 0.54075927] 1 False\n",
      "[0.6489117  0.35108823] 0 False\n",
      "[0.5259268  0.47407308] 1 False\n",
      "[0.7059986 0.2940014] 0 False\n",
      "[0.5925148  0.40748516] 0 False\n",
      "[0.46278575 0.5372142 ] 1 False\n",
      "[0.6464211  0.35357895] 1 False\n",
      "[0.79755926 0.20244072] 0 False\n",
      "[0.7089356 0.2910644] 0 False\n",
      "[0.5969938  0.40300626] 0 False\n",
      "[0.45904782 0.54095215] 1 False\n",
      "[0.64740115 0.35259885] 0 False\n",
      "[0.5140215  0.48597848] 0 False\n",
      "[0.35775548 0.6422446 ] 1 False\n",
      "[0.5518392  0.44816083] 0 False\n",
      "[0.39139262 0.6086074 ] 1 False\n",
      "[0.5818702 0.4181298] 0 False\n",
      "[0.4188391 0.5811609] 0 False\n",
      "[0.25964522 0.7403548 ] 0 False\n",
      "[0.14088936 0.85911065] 1 False\n",
      "[0.24785222 0.7521478 ] 0 False\n",
      "[0.12907845 0.8709215 ] 0 False\n",
      "[0.06039288 0.93960714] 1 False\n",
      "[0.10462683 0.89537317] 1 False\n",
      "[0.17486365 0.8251363 ] 1 False\n",
      "[0.28089982 0.7191001 ] 0 False\n",
      "[0.13938221 0.86061776] 0 False\n",
      "[0.06241051 0.9375895 ] 1 False\n",
      "[0.09858261 0.9014174 ] 1 False\n",
      "[0.15323271 0.8467673 ] 1 False\n",
      "[0.23342806 0.766572  ] 0 False\n",
      "[0.10757233 0.8924277 ] 1 False\n",
      "[0.16234496 0.837655  ] 1 False\n",
      "[0.23791578 0.7620842 ] 1 False\n",
      "[0.33051607 0.6694839 ] 1 False\n",
      "[0.43755397 0.562446  ] 0 False\n",
      "[0.2578849  0.74211514] 1 False\n",
      "[0.35170788 0.6482921 ] 1 False\n",
      "[0.47551963 0.5244804 ] 1 False\n",
      "[0.61988086 0.38011914] 1 False\n",
      "[0.7427332  0.25726682] 0 False\n",
      "[0.5723089 0.4276911] 0 False\n",
      "[0.3741591  0.62584084] 0 False\n",
      "[0.2226678 0.7773322] 1 False\n",
      "[0.32241172 0.6775883 ] 1 False\n",
      "[0.4611434  0.53885657] 0 False\n",
      "[0.28631032 0.7136897 ] 1 False\n",
      "[0.41803995 0.5819601 ] 1 False\n",
      "[0.57920486 0.4207951 ] 1 False\n",
      "[0.7233772 0.2766227] 0 False\n",
      "[0.56988955 0.43011048] 0 False\n",
      "[0.39504328 0.6049567 ] 0 False\n",
      "[0.2504068  0.74959326] 1 False\n",
      "[0.37891614 0.62108386] 0 False\n",
      "[0.24022278 0.75977725] 1 False\n",
      "[0.3641211  0.63587886] 1 False\n",
      "[0.5270474  0.47295257] 0 False\n",
      "[0.3645875 0.6354125] 0 False\n",
      "[0.23418707 0.765813  ] 1 False\n",
      "[0.35671324 0.6432867 ] 0 False\n",
      "[0.22917135 0.77082866] 1 False\n",
      "[0.34886378 0.6511362 ] 0 False\n",
      "[0.22398627 0.7760137 ] 1 False\n",
      "[0.3409008  0.65909916] 1 False\n",
      "[0.4964208 0.5035792] 1 False\n",
      "[0.65410435 0.34589565] 1 False\n",
      "[0.7815524  0.21844767] 1 False\n",
      "[0.87172824 0.1282717 ] 0 False\n",
      "[0.81298965 0.18701035] 0 False\n",
      "[0.7367304  0.26326966] 0 False\n",
      "[0.63975704 0.360243  ] 1 False\n",
      "[0.77786654 0.22213347] 1 False\n",
      "[0.8743267  0.12567323] 0 False\n",
      "[0.82758534 0.17241465] 0 False\n",
      "[0.77016234 0.22983773] 0 False\n",
      "[0.70659024 0.2934098 ] 0 False\n",
      "[0.63027364 0.36972633] 1 False\n",
      "[0.76770407 0.23229595] 0 False\n",
      "[0.7056638  0.29433617] 0 False\n",
      "[0.6331884 0.3668116] 0 True\n"
     ]
    }
   ],
   "source": [
    "###test\n",
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    action_prob = model.get_action(state.reshape(1,-1))\n",
    "    action = np.random.choice([x for x in range(env.action_space.n)], p = action_prob[0])\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(action_prob[0],action,done)\n",
    "    state = next_state     \n",
    "    score += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Actor(env.observation_space.shape[0],env.action_space.n) #model = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5016776, 0.4983225]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_action_select = K.sum(tf.one_hot(before_action,depth=2) * now_action_prob,axis=-1)\n",
    "\n",
    "ratio = K.exp(K.log(now_action_select) - K.log(before_action_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = K.constant([1,0,1,0,0],dtype = 'int64')\n",
    "test_index = K.reshape(test_index,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_action = K.constant([[0.1,0.2],[0.3,0.4],[0.5,0.6],[0.7,0.8],[0.9,1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = K.constant([0,0,0,0,0],dtype = 'int64')\n",
    "test_index = K.reshape(test_index,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'one_hot_6:0' shape=(5, 1, 3) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.3, 0.6, 0.7, 0.9], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(K.sum(tf.squeeze(tf.one_hot(test_index,depth=2),axis=1)* test_action,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(test_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
