{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sh2\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import keras.layers as layers\n",
    "import keras.optimizers as optimizers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self,input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.__make_network()\n",
    "        self.__make_loss_function()\n",
    "        \n",
    "    def __make_network(self):\n",
    "        input_layer = layers.Input(shape=(self.input_shape,))\n",
    "        x = layers.Dense(256,activation = 'relu')(input_layer)\n",
    "        x = layers.Dense(1)(x)\n",
    "        self.model = Model(inputs = input_layer, outputs = x)\n",
    "    def get_value(self,state):\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def __make_loss_function(self):\n",
    "        HUBER_DELTA = 0.5\n",
    "        value_output = self.model.output\n",
    "        reward_placeholder = K.placeholder(shape=(None,1),name = 'reward')\n",
    "        #loss = K.abs(reward_placeholder - value_output)\n",
    "        #loss = K.switch(loss < HUBER_DELTA, 0.5 * loss ** 2 , HUBER_DELTA * (loss - 0.5 * HUBER_DELTA))\n",
    "        #loss = K.sum(loss)\n",
    "        loss = K.mean(K.square(reward_placeholder - value_output))\n",
    "        \n",
    "        optimizer = optimizers.Adam(learning_rate)\n",
    "        update = optimizer.get_updates(loss =loss, params = self.model.trainable_weights)\n",
    "        \n",
    "        self.update_function = K.function(inputs = [self.model.input,\\\n",
    "                                                   reward_placeholder],\\\n",
    "                                         outputs = [] , updates = update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self,input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        self.__make_network()\n",
    "        self.__make_loss_function()\n",
    "        \n",
    "    def __make_network(self):\n",
    "        input_layer = layers.Input(shape=(self.input_shape,))    \n",
    "        x = layers.Dense(256, activation = 'relu')(input_layer)\n",
    "        x = layers.Dense(self.output_shape, activation='softmax')(x)\n",
    "        self.model = Model(inputs = input_layer, outputs = x)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def __make_loss_function(self):\n",
    "        before_action_prob = K.placeholder(shape = (None, 1),\\\n",
    "                                          name = 'before_action_prob')\n",
    "        before_action = K.placeholder(shape = (None, 1),\\\n",
    "                                          name = 'before_action',dtype = 'int64') ########\n",
    "\n",
    "        advantage = K.placeholder(shape = (None,1), name ='advantage')\n",
    "        \n",
    "        now_action_prob = self.model.output\n",
    "        #now_action_prob = tf.gather_nd(now_action_prob,before_action)\n",
    "        now_action_prob = tf.gather(now_action_prob,before_action,axis=1)\n",
    "        \n",
    "        ratio = K.exp(K.log(before_action_prob) - K.log(now_action_prob))\n",
    "        \n",
    "        surr_1 = ratio * advantage\n",
    "        surr_2 = K.clip(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "        loss = - K.min((surr_1,surr_2))\n",
    "        optimizer = optimizers.Adam(lr = learning_rate)\n",
    "        updates = optimizer.get_updates(loss = loss, params = self.model.trainable_weights)\n",
    "        \n",
    "        self.update_function = K.function(inputs = [self.model.input,before_action,\\\n",
    "                                       before_action_prob,advantage],\\\n",
    "                            outputs = [],\\\n",
    "                            updates = updates)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, input_shape,output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        self.actor = Actor(input_shape,output_shape)\n",
    "        self.value = Value(input_shape)\n",
    "    \n",
    "        self.memory = []\n",
    "    def put_data(self,data):\n",
    "        self.memory.append(data)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        return self.actor.model.predict(state)\n",
    "    def memory_to_trainable(self):\n",
    "        state_list, action_list, reward_list, next_state_list, prob_list, done_list = [],\\\n",
    "        [], [], [], [], []\n",
    "        \n",
    "        for data in self.memory:\n",
    "            state, action, reward, next_state, prob, done = data\n",
    "            \n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            reward_list.append([reward])\n",
    "            next_state_list.append(next_state)\n",
    "            #print(prob) 이거 리스트인지확인해야함\n",
    "            prob_list.append(prob)\n",
    "            done = 0 if done else 1\n",
    "            done_list.append([done])\n",
    "        return np.array(state_list), np.array(action_list), np.array(reward_list),\\\n",
    "                np.array(next_state_list), np.array(prob_list), np.array(done_list)     \n",
    "        \n",
    "    def train(self):\n",
    "        state,action,reward,next_state,done_mask,prob = self.memory_to_trainable()\n",
    "        done_mask = done_mask.reshape(-1,1)\n",
    "        for i in range(env_iteration_number):\n",
    "            td_error = reward + gamma * self.value.get_value(next_state) * done_mask\n",
    "            delta = np.array(td_error - self.value.get_value(state))\n",
    "            advantage_list = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_list.append(advantage)\n",
    "            advantage_list.reverse()\n",
    "            advantage = np.array(advantage_list).reshape(-1,1)\n",
    "            self.actor.update_function([state,action,prob,advantage])\n",
    "            self.value.update_function([state,advantage])\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env_iteration_number = 3\n",
    "learning_rate = 0.0005\n",
    "lmbda =0.95\n",
    "eps_clip = 0.1\n",
    "epochs = 100\n",
    "T_horizon = 20\n",
    "gamma         = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Agent(env.observation_space.shape[0],env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20  episode average :  24.5\n",
      "20  episode average :  23.4\n",
      "20  episode average :  20.5\n",
      "20  episode average :  24.25\n",
      "20  episode average :  19.85\n",
      "20  episode average :  20.9\n",
      "20  episode average :  22.15\n",
      "20  episode average :  27.2\n",
      "20  episode average :  26.05\n",
      "20  episode average :  18.3\n",
      "20  episode average :  24.15\n",
      "20  episode average :  22.4\n",
      "20  episode average :  16.8\n",
      "20  episode average :  23.6\n",
      "20  episode average :  20.55\n",
      "20  episode average :  26.0\n",
      "20  episode average :  22.0\n",
      "20  episode average :  16.7\n",
      "20  episode average :  19.4\n",
      "20  episode average :  21.85\n",
      "20  episode average :  24.2\n",
      "20  episode average :  23.05\n",
      "20  episode average :  22.9\n"
     ]
    }
   ],
   "source": [
    "check = 20\n",
    "score = 0\n",
    "\n",
    "for iterate in range(2000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        for t in range(T_horizon):\n",
    "            action_prob = model.get_action(state.reshape(1,-1))\n",
    "\n",
    "            action = np.random.choice([x for x in range(env.action_space.n)], p = action_prob[0])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            model.put_data((state,action,reward/100.0, next_state,action_prob[0][action],done))\n",
    "            state = next_state\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        model.train()\n",
    "    if (iterate % 20 == 0) & (iterate != 0) :\n",
    "        print(check, \" episode average : \", score / check) \n",
    "        score =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
